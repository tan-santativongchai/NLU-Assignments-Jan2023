{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification using Pytorch and Torchtext\n",
    "\n",
    "- bidirectional LSTM on some news\n",
    "- some labeled data on news....\n",
    "\n",
    "- Torchtext to help us numericalize and load some data\n",
    "- Torchtext is backed by PyTorch....so naturally torchtext is quite good\n",
    "- Torchtext is not meant to replace spacy....spacy is still like better in general\n",
    "  \n",
    "- PyTorch to help us make some neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchtext\n",
    "from torch import nn\n",
    "\n",
    "import time\n",
    "\n",
    "#1. puffer - it's outdated....\n",
    "#2. spend some money - 300 baht get collab pro\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#reproducibility \n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.0'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchdata.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "\n",
    "Make our life easy by using some ready-to-be-used dataset by torchtext\n",
    "\n",
    "- in your assignment, i will ask you to use penn treebank....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are using puffer\n",
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "# from torchtext.datasets import AG_NEWS\n",
    "# train, test = AG_NEWS()\n",
    "import csv\n",
    "data_dp_train = open(\"train.txt\", encoding=\"utf-8\")\n",
    "data_dp_test = open(\"test.txt\", encoding=\"utf-8\")\n",
    "train = csv.reader(data_dp_train, skipinitialspace=True)\n",
    "test = csv.reader(data_dp_test, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x7f8b88a4a2e0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train  #a new object by torchdata.....streaming data (yield ....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - exploratory data analysis\n",
    "- check common words\n",
    "- look at some random sample....how it looks, so that we can design proper neural network\n",
    "- visualize statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Class Index', 'Title', 'Description']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))  #generator\n",
    "# (“World”, “Sports”, “Business”, “Sci/Tech”)\n",
    "#  1,        2,        3,          4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4',\n",
       " 'Comets, Asteroids and Planets around a Nearby Star (SPACE.com)',\n",
       " 'SPACE.com - A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iter(train))[100]  #generator\n",
    "# (“World”, “Sports”, “Business”, “Sci/Tech”)\n",
    "#  1,        2,        3,          4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([y for y, x in list(iter(train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x7f8b88a4a2e0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_csv.reader' object has no attribute 'random_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114/83214856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#i gonna cheat a little bit, not gonna use all...my fans will work too hard....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m too_much, train, valid = train.random_split(total_length=train_size, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                             weights = {\"too_much\": 0.7, \n\u001b[1;32m      4\u001b[0m                                                        \u001b[0;34m\"smaller_train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                        \"valid\": 0.1},\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_csv.reader' object has no attribute 'random_split'"
     ]
    }
   ],
   "source": [
    "#i gonna cheat a little bit, not gonna use all...my fans will work too hard....\n",
    "too_much, train, valid = train.random_split(total_length=train_size, \n",
    "                                            weights = {\"too_much\": 0.7, \n",
    "                                                       \"smaller_train\": 0.2,\n",
    "                                                       \"valid\": 0.1},\n",
    "                                            seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "val_size   = len(list(iter(valid)))\n",
    "test_size  = len(list(iter(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 12000, 7600)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 15:10:17.852008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## 3.1 Tokenizing\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "#check whether the tokenizer works.....\n",
    "# tokens    = tokenizer(\"Chaky likes deep learning very much and wants his \\\n",
    "                    #   student to be number 1 in Asia....\")\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 'Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 Numericalization\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data_iter):  #data_iter, e.g., train\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>',\n",
    "                                                                 '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[\"<unk>\"]) #if you don't the id of this word, set it unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 944, 38, 3956, 8, 43, 498, 109, 6]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['Chaky', 'wants', 'his', 'student', 'to', 'be', 'number', '1', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52686"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)  #52k unique words....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FastText embedding\n",
    "\n",
    "We gonna insert this embedding to the NN on the fly....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52686, 300])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape #(vocab size, 300) == (52k, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0935,  0.0915,  0.2640,  0.0387,  0.0843,  0.3809, -0.1776,  0.1745,\n",
       "        -0.0362, -0.0278])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please lookup the fasttext embedding of id 100\n",
    "fast_embedding[100][:10] #size of 300 dim of this word id 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparing dataloader\n",
    "\n",
    "Optional - you can either make your own batch loader....\n",
    "You can use pytorch dataloader...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  #1, 2, 3, 4 ---> 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhy padding????\\n\\nin the same batch, e.g., batch size = 2\\n\\n\"chaky eat sushi\", ==> \"chaky\", \"eat\", \"sushi\" ==> 0, 22, 11, 1, 1\\n\"chaky sleep\" ==> \"chaky\", \"sleep\" ==> 0, 99, 1, 1, 1\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "why padding????\n",
    "\n",
    "in the same batch, e.g., batch size = 2\n",
    "\n",
    "\"chaky eat sushi\", ==> \"chaky\", \"eat\", \"sushi\" ==> 0, 22, 11, 1, 1\n",
    "\"chaky sleep\" ==> \"chaky\", \"sleep\" ==> 0, 99, 1, 1, 1\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence #making each batch same length\n",
    "\n",
    "pad_ix = vocab['<pad>']\n",
    "\n",
    "#this function gonna be called by DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))  #[3, 1, 0, 2, ]\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) #[0, 44, 21, 2]\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0)) #for padding\n",
    "        \n",
    "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
    "        pad_sequence(text_list, padding_value=pad_ix, batch_first=True), \\\n",
    "        torch.tensor(length_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "val_loader   = DataLoader(valid, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "test_loader  = DataLoader(test, batch_size = batch_size,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label, text, length in train_loader:\n",
    "#     break\n",
    "\n",
    "#label: [batch size, ]\n",
    "#text : [batch size, longest length of this batch] ==> [batch size, seq len] ==> [b, l]\n",
    "#length:[batch size, ]\n",
    "\n",
    "# label, text, length  #why we need length --> we can later ignore padding...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Designing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, \n",
    "                 dropout):\n",
    "        #input dim = how many vocab you have\n",
    "        #emb dim = 300 --> we use fasttext\n",
    "        #padding_idx tells this lookup table to ignore, and just randomize....\n",
    "        #<unk>, <bos>, <eos>\n",
    "        self.embedding_layer = nn.Embedding(input_dim, emb_dim, padding_idx=pad_ix)\n",
    "        self.lstm            = nn.LSTM(emb_dim,\n",
    "                                       hid_dim,\n",
    "                                       num_layers = num_layers,\n",
    "                                       bidrectional = bidirectional,\n",
    "                                       dropout = dropout,  #dropout is applied between layers....\n",
    "                                       batch_first=True)\n",
    "        \n",
    "        self.fc              = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        #x: [batch size, seq len]\n",
    "        \n",
    "        embedded_x = self.embedding_layer(x)\n",
    "        #x: [batch size, seq len, emb dim]\n",
    "        \n",
    "        #pack this embedded_x in such a way that RNN knows to ignore padding....\n",
    "        #without batch_first = True; things will become [seq len, batch size, emb dim]\n",
    "        pack_embedded = nn.utils.rnn.pack_padded_sequence(embedded_x, lengths.to('cpu'),\n",
    "                                                          enforce_sorted=False,\n",
    "                                                          batch_first = True\n",
    "                                                          )\n",
    "        \n",
    "        #packed_outputs is basically all hidden states\n",
    "        #h is the last hidden state\n",
    "        #c is the last cell state\n",
    "        packed_outputs, (h, _) = self.lstm(pack_embedded)\n",
    "        \n",
    "        #h: [num_layers * num_directions, batch_size, hidden dim]\n",
    "        \n",
    "        #it happens that because packed_outputs is all hidden states....some hidden states near the end is\n",
    "        #hidden state for padding, pytorch guys help you\n",
    "        #by using this pad_packed_sequence, then all the hidden states will only be not padding....\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first = True)\n",
    "        #output: [batch size, seq len, direction * hidden sim]\n",
    "        \n",
    "        #last hidden state - concat last forward and backward states\n",
    "        last_hidden_state = torch.cat((h[-1, :, :], h[-2, :, :]), dim = 1)\n",
    "        #last_hidden_state: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        #for sentiment analysis.....what should i sent to my linear layer...\n",
    "        return self.fc(last_hidden_state)  #[batch_size, output_dim]==> [batch_size, 4]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note:\n",
    "I have try running on puffer but I cannot. (also my pc have problem with libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
