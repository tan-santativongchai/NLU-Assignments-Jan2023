# Readings Assignment
Here are my lectionaries:</br>
### 1. The Geometry of Multilingual Language Model Representations
#### Source: https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.9/
||Contents|
|---|---|
|Related Works|Mean representation distances between languages correlate with phylogenetic distances between languages (Rama et al., 2020), and individual representations can be used to predict linguistic typological features (Choenni and Shutova, 2020), particularly after projecting onto language-sensitive subspaces (Liang et al., 2021). The models also maintain language-neutral subspaces that encode information that is shared across languages. Syntactic information is encoded largely within a shared syntactic subspace (Chi et al., 2020), token frequencies may be encoded similarly across languages (Rajaee and Pilehvar, 2022), and representations shifted according to language means facilitate cross-lingual parallel sentence retrieval (Libovick√Ω et al., 2020; Pires et al., 2019)|
|Method|- used the pre-trained language model XLM-R. <br/>- XLM-R follows the Transformer architecture of BERT and RoBERTa<br/>- Model is trained to predict masked tokens in 100 languages.<br/>- Inputted text sequences from the OSCAR corpus of cleaned web text data to extract contextualized token representations from XLM-R<br/>- Inputted text sequences from the OSCAR corpus of cleaned web text data.<br/>- Used the vector representations outputted by each Transformer layer as our token representations in layers one through twelve.<br/>- Used the uncontextualized token embeddings for layer zero.<br/>- Affine subspaces accounted for language modeling performance. To assess the extent to which affine subspaces encoded relevant information in their corresponding languages.<br/>- Affine subspaces accounted for language modeling performance to directly quantified distances between the subspaces themselves. <br/>- To assess the extent to which affine subspaces encoded relevant information in their corresponding languages.<br/>- Then, do either Language-sensitive axes or Language-neutral axes.|
|Result|88 considered languages were skewed substantially towards Indo-European languages; 52 of the 88 languages were in some subfamily of the Indo-European languages|
